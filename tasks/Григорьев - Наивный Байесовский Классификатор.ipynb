{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f915973",
   "metadata": {},
   "source": [
    "Наивный байесовский классификатор — простой вероятностный классификатор, основанный на применении теоремы Байеса со строгими предположениями о независимости.\n",
    "\n",
    "В зависимости от точной природы вероятностной модели, наивные байесовские классификаторы могут обучаться очень эффективно. Во многих практических приложениях для оценки параметров для наивных байесовых моделей используют метод максимального правдоподобия; другими словами, можно работать с наивной байесовской моделью, не веря в байесовскую вероятность и не используя байесовские методы.\n",
    "\n",
    "Несмотря на наивный вид и очень упрощенные условия, наивные байесовские классификаторы часто работают намного лучше нейронных сетей во многих сложных жизненных ситуациях.\n",
    "\n",
    "Достоинством наивного байесовского классификатора является малое количество данных, необходимых для обучения, оценки параметров и классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f801478f",
   "metadata": {},
   "source": [
    "Теория:\n",
    "У нас есть некоторый объект, который относится к какому-либо классу, и нам нужно вычислить, к какому классу вероятнее всего относится объект.<img src=\"img/91f7472c5d78cd4ac9d038ee807f5632.png\">  Пользуясь теоремой Байеса, нахождение P(C|O) переходит к косвенным вероятностям.<img src=\"img/5f0ba16fae1e4613485d29e1fa62a4e4.png\"> Выделяя свойства объектов, можно перейти к вероятностям принадлежности свойств. <img src=\"img/e924d263b59e77a4026a48dab0cad9ae.png\">![title](img/e924d263b59e77a4026a48dab0cad9ae.png) Принимая, что свойства независимы друг от друга и зависят только от класса, можно перейти к формуле: <img src=\"img/91f7472c5d78cd4ac9d038ee807f5632.png\">![title](img/7c0af694411c5f3dc7174d9a027759b3.png) И тогда финальная формула будет выглядеть так: <img src=\"img/91f7472c5d78cd4ac9d038ee807f5632.png\">![title](img/f6017ae3be1985b9e9900920c05659d3.png) Что сведет задачу к вычислению вероятностей P(C) и P(O|C)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0402cf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class NaiveBayesClassifier(object):\n",
    "    def __init__(self):  \n",
    "        self.__class_freq = defaultdict(lambda:0)\n",
    "        self.__feat_freq = defaultdict(lambda:0)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        #считаем классы и частоты особенностей\n",
    "        for feature, label in zip(X, y):\n",
    "            self.__class_freq[label] += 1\n",
    "            for value in feature:\n",
    "                self.__feat_freq[(value, label)] += 1\n",
    "\n",
    "        #нормализуем значения\n",
    "        num_samples = len(X)\n",
    "        \n",
    "        #P(C)\n",
    "        for k in self.__class_freq:\n",
    "            self.__class_freq[k] /= num_samples\n",
    "        \n",
    "        #P(O|C)\n",
    "        for value, label in self.__feat_freq:\n",
    "            self.__feat_freq[(value, label)] /= self.__class_freq[label]\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        #получаем argmin(-log(C|O))\n",
    "        return min(self.__class_freq.keys(), \n",
    "                   key=lambda c : self.__calculate_class_freq(X, c)) \n",
    "\n",
    "    def __calculate_class_freq(self, X, clss):\n",
    "        #считаем частоту для заданного класса\n",
    "        freq = - np.log(self.__class_freq[clss])\n",
    "\n",
    "        for feat in X: \n",
    "            freq += - np.log(self.__feat_freq.get((feat, clss), 10 ** (-7)))\n",
    "        return freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81821f04",
   "metadata": {},
   "source": [
    "Переход к логарифмам и добавление 10^(-7) осуществляется для избежания умножения на 0 и избегания слишком маленьких вероятностей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "532655bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7456140350877193\n"
     ]
    }
   ],
   "source": [
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = NaiveBayesClassifier().fit(X_train, y_train)\n",
    "\n",
    "predictions = [model.predict(x) for x in X_test]\n",
    "print(accuracy_score(predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d758a1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = NaiveBayesClassifier().fit(X_train, y_train)\n",
    "\n",
    "predictions = [model.predict(x) for x in X_test]\n",
    "print(accuracy_score(predictions, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
